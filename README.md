# LLaMA-3.1-8B-AGNews-SFT

æ¨¡å‹æƒé‡ [ğŸ¤—Hugging Face](https://huggingface.co/Word2Li/LLaMA-3.1-8B-AGNews-SFT)

## æ¦‚è¦

- Base Modelï¼š[LLaMA 3.1 8B](https://ai.meta.com/blog/meta-llama-3-1)
- datasetï¼š[AG News](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)
- è®­ç»ƒæ¡†æ¶ï¼š[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

## å¿«é€Ÿä¸Šæ‰‹

1. å®‰è£… LLaMA-Factory åŠç›¸å…³ä¾èµ–ï¼Œå‚è€ƒé¡¹ç›® [repo](https://github.com/hiyouga/LLaMA-Factory)

1. å…‹éš†æœ¬é¡¹ç›®

    ```bash
    git clone https://github.com/Word2VecT/ LLaMA-3.1-8B-AGNews-SFT
    ```

1. å°†æœ¬é¡¹ç›® `data` æ–‡ä»¶å¤¹æ›¿æ¢ LLaMA-Factory çš„ `data` æ–‡ä»¶å¤¹ï¼Œæˆ–å‚è€ƒ LLaMA-Factory è¯´æ˜æ·»åŠ æ•°æ®é›†ä¿¡æ¯

1. å®‰è£… Flash Attention 2ï¼Œå‚è€ƒé¡¹ç›® [repo](https://github.com/Dao-AILab/flash-attention)

1. å¼€å§‹è®­ç»ƒï¼ˆæ³¨æ„æ›¿æ¢è¾“å‡ºè·¯å¾„ï¼Œæ¨¡å‹è·¯å¾„å¯ä»¥æ›¿æ¢ä¸ºå›½å†…[é­”å¡”ç¤¾åŒº](https://modelscope.cn/models)ï¼Œè¯¦ç»†è§ LLaMA-Factory è¯´æ˜ï¼‰

    ```bash
    torchrun --nnodes=1 --nproc-per-node=8 src/train.py \
    --deepspeed examples/deepspeed/ds_z3_config.json \
    --stage sft \
    --do_train \
    --use_fast_tokenizer \
    --flash_attn fa2\
    --model_name_or_path meta-llama/Llama-3.1-8B \
    --dataset ag_news_train \
    --template llama3 \
    --finetuning_type full \
    --output_dir /path/to/your/model \
    --overwrite_cache \
    --overwrite_output_dir \
    --warmup_ratio 0.03 \
    --weight_decay 0. \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --ddp_timeout 9000 \
    --learning_rate 2e-5 \
    --lr_scheduler_type cosine \
    --cutoff_len 4096 \
    --save_steps 2000 \
    --logging_steps 1 \
    --plot_loss \
    --resize_vocab \
    --num_train_epochs 1 \
    --bf16 \
    --report_to wandb \
    --run_name llama3.1-8B-agnews-steps4
    ```

1. è®­ç»ƒç»“æŸåè¿è¡Œæ‰¹é‡æ¨ç†

    ```bash
    llamafactory-cli train /path/to/your/LLaMA-3.1-eval.yaml
    ```

1. è¿è¡Œ eval è„šæœ¬ï¼ˆå…¶ä»–å¯è§†åŒ–è„šæœ¬åŒç†ï¼‰

    ```bash
    python /path/to/eval/eval.py
    ```

1. Enjoy!

## è¯„æµ‹

| åˆ†ç±»         | å‡†ç¡®ç‡   | ç²¾ç¡®ç‡   | å¬å›ç‡   | F1 Score  |
|--------------|----------|----------|----------|-----------|
| World        | 95.95%  | 96.87%  | 95.95%  | 96.40%   |
| Sports       | 99.42%  | 99.00%  | 99.42%  | 99.21%   |
| Business     | 91.53%  | 93.95%  | 91.53%  | 92.72%   |
| Sci/Tech     | 94.84%  | 91.99%  | 94.84%  | 93.39%   |
| æ€»ä½“ï¼ˆå®å¹³å‡ï¼‰ | 95.43%  | 95.45%  | 95.43%  | 95.43%   |

## åˆ†ç±»æ•ˆæœ

![åˆ†ç±»æ•ˆæœ](eval/classification_tsne_with_incorrect.png)
